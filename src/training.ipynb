{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr4MOUR9JWrl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_oYpaJzRzPq"
      },
      "outputs": [],
      "source": [
        "#-----------------------DEFINING FUNCTIONS FOR MODEL----------------------------\n",
        "def conv_block(inputs, num_filters):\n",
        "  x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "\n",
        "  x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "  x = conv_block(inputs, num_filters)\n",
        "  p = MaxPool2D((2, 2))(x)\n",
        "  p = Dropout(0.5)(p)\n",
        "  return x, p\n",
        "\n",
        "def decoder_block(inputs, skip, num_filters):\n",
        "  x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "  x = Concatenate()([x, skip])\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = conv_block(x, num_filters)\n",
        "  return x\n",
        "\n",
        "def build_unet(input_shape):\n",
        "  inputs = Input(input_shape)\n",
        "\n",
        "  s1, p1 = encoder_block(inputs, 64)\n",
        "  s2, p2 = encoder_block(p1, 128)\n",
        "  s3, p3 = encoder_block(p2, 256)\n",
        "  s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "  b1 = conv_block(p4, 1024)\n",
        "\n",
        "  d1 = decoder_block(b1, s4, 512)\n",
        "  d2 = decoder_block(d1, s3, 256)\n",
        "  d3 = decoder_block(d2, s2, 128)\n",
        "  d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "  outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "  model = Model(inputs, outputs, name=\"UNET\")\n",
        "  return model\n",
        "\n",
        "def load_data(path):\n",
        "  train_x = sorted(glob(os.path.join(path, \"train\", \"images\", \"*\")))\n",
        "  print(f\"Train Images size: {len(train_x)}\")\n",
        "  train_y = sorted(glob(os.path.join(path, \"train\", \"masks\", \"*\")))\n",
        "  print(f\"Train Masks size: {len(train_y)}\")\n",
        "\n",
        "  valid_x = sorted(glob(os.path.join(path, \"valid\", \"images\", \"*\")))\n",
        "  print(f\"Valid Images size: {len(valid_x)}\")\n",
        "  valid_y = sorted(glob(os.path.join(path, \"valid\", \"masks\", \"*\")))\n",
        "  print(f\"Valid Masks size: {len(valid_y)}\")\n",
        "\n",
        "  return (train_x, train_y), (valid_x, valid_y)\n",
        "\n",
        "def read_image(path):\n",
        "  path = path.decode()\n",
        "  x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "  x = cv2.resize(x, (224, 224))\n",
        "  x = x/255.0 # normalizing pixels\n",
        "  x = np.expand_dims(x, axis=-1)\n",
        "  return x\n",
        "\n",
        "def read_mask(path):\n",
        "  path = path.decode()\n",
        "  x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "  x = cv2.resize(x, (224, 224))\n",
        "  x = x/255.0 # normalizing mask\n",
        "  x = np.expand_dims(x, axis=-1)\n",
        "  return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "  def _parse(x, y):\n",
        "    x = read_image(x)\n",
        "    y = read_mask(y)\n",
        "    return x, y\n",
        "\n",
        "  x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n",
        "  x.set_shape([height, width, 1]) # 1 for grayscale\n",
        "  y.set_shape([height, width, 1]) # 1 for grayscale\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def tf_dataset(x, y, batch=8):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch)\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "#------------------------ADDRESSING WEIGHT IMBALANCE----------------------------\n",
        "def add_sample_weights(image, label):\n",
        "  # pos, neg, total values from running calculating_weights file\n",
        "  pos, neg, total = 7933164, 146608916, 154542080\n",
        "\n",
        "  # scaling by total/2 helps keep the loss to a similar magnitude\n",
        "  # the sum of the weights of all examples stays the same\n",
        "  weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "  weight_for_1 = (1 / pos) * (total / 2.0) - 4\n",
        "\n",
        "  print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "  print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "\n",
        "  class_weight = tf.constant([weight_for_0, weight_for_1])\n",
        "  class_weight = class_weight/tf.reduce_sum(class_weight)\n",
        "\n",
        "  # create an image of 'sample_weights' by using the label at each pixel\n",
        "  # as an index into the 'class_weight'\n",
        "  sample_weights = tf.gather(class_weight, indices=tf.cast(label, tf.int32))\n",
        "\n",
        "  return image, label, sample_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHczMIxWqxdx"
      },
      "outputs": [],
      "source": [
        "\n",
        "#---------------------INITIALIZATION FOR TRAINING MODEL-------------------------\n",
        "unet_name = \"your-model-name-here\"\n",
        "dataset_path = \"/content/drive/MyDrive/Images\"\n",
        "files_dir = \"/content/drive/MyDrive/Files\" # where to save model, and training log\n",
        "\n",
        "# if you want to try multiple epochs and batch sizes, add to the following arrays\n",
        "epochs = [100]\n",
        "batch_size = [8]\n",
        "(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "METRICS = [\n",
        "  tf.keras.metrics.TruePositives(name='tp'),\n",
        "  tf.keras.metrics.FalsePositives(name='fp'),\n",
        "  tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "  tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "  tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "  tf.keras.metrics.Precision(name='precision'),\n",
        "  tf.keras.metrics.Recall(name='recall'),\n",
        "  tf.keras.metrics.AUC(name='auc'),\n",
        "  tf.keras.metrics.AUC(name='prc', curve='PR') # precision-recall curve\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1LGMdqvHEhM"
      },
      "outputs": [],
      "source": [
        "#---------------TRAINING FOR DIFFERENT EPOCHS AND BATCH SIZES-------------------\n",
        "for e in epochs:\n",
        "  for sz in batch_size:\n",
        "    #-------------------------INITIALIZING FILES ETC----------------------------\n",
        "    tf.keras.backend.clear_session()\n",
        "    height, width = 224, 224\n",
        "\n",
        "    print(f\"\\n\\n* * * * * Epochs: {e} | Batch Size: {sz} * * * * *\\n\\n\")\n",
        "    model_file = os.path.join(files_dir, \"models\", f\"{unet_name}.h5\")\n",
        "    log_file = os.path.join(files_dir, \"logs\", f\"log-{unet_name}.csv\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch=sz)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch=sz)\n",
        "\n",
        "    input_shape = (height, width, 1)\n",
        "\n",
        "    model = build_unet(input_shape)\n",
        "    # if you want to further train a pretrained model,\n",
        "    # uncomment the line below (no need to use build_unet),\n",
        "    # and add your model name & path\n",
        "    # model = tf.keras.models.load_model('/content/drive/MyDrive/Files/models/model.h5', compile=False)\n",
        "\n",
        "    #-----------------------------TRAINING MODEL--------------------------------\n",
        "    opt = tf.keras.optimizers.Adam()\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer=opt, metrics=METRICS)\n",
        "\n",
        "    callbacks=[\n",
        "        ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n",
        "        CSVLogger(log_file)\n",
        "    ]\n",
        "\n",
        "    with tf.device(tf.test.gpu_device_name()):\n",
        "      model.fit(\n",
        "            train_dataset.map(add_sample_weights),\n",
        "            validation_data=valid_dataset,\n",
        "            epochs=e,\n",
        "            callbacks=callbacks,\n",
        "            workers=4,\n",
        "            verbose=2\n",
        "      )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}